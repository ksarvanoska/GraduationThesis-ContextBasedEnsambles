{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "context_based_rand_forest.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "90O-KH4ztaIQ"
      },
      "source": [
        "!pip install -q arff"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pI6Xb9wzurRY"
      },
      "source": [
        "!rm -rf 'bank-additional-full'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiZ3HlqXtNgr"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import arff\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import statsmodels.api as sm\n",
        "import random\n",
        "import matplotlib.pyplot as plot\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import pickle \n",
        "from sklearn import preprocessing\n",
        "import shutil\n",
        "from scipy.io import arff\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuBBb1J34qZV"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29uIGUkKtQBR"
      },
      "source": [
        "class ContextBasedRandomForestClassifierEnsemble:  \n",
        "\n",
        "    \n",
        "    #user should not see these\n",
        "    def __init__(self, target_feature, files_path):\n",
        "        self.target_feature = target_feature\n",
        "        self.files_path = os.getcwd() + '/' + files_path\n",
        "        self.models = {}\n",
        "        self.numerical_contexts = []\n",
        "        self.non_numeric_contexts = []\n",
        "        self.context_models = {}\n",
        "        self.contexts = []\n",
        "        self.intervals = {}\n",
        "        self.encoders = {}\n",
        "    \n",
        "    def set_numerical_contexts(self, numerical_contexts = []):\n",
        "        self.numerical_contexts = numerical_contexts\n",
        "             \n",
        "    def set_non_numeric_contexts(self, non_numeric_contexts):\n",
        "        self.non_numeric_contexts = non_numeric_contexts\n",
        "        \n",
        "    def set_contexts(self, df, manually = False, contexts=[]):\n",
        "        if(manually):\n",
        "            self.contexts = contexts\n",
        "            return\n",
        "\n",
        "        self.contexts = list(df.columns.values)\n",
        "        self.contexts.remove(self.target_feature)\n",
        "        for feature in self.contexts:\n",
        "            if np.issubdtype(df[feature].dtype, np.number):\n",
        "                self.numerical_contexts.append(feature)\n",
        "            else:\n",
        "                self.non_numeric_contexts.append(feature)\n",
        "                \n",
        "    \n",
        "    \n",
        "    #new_dataset_path is the path from the current directory\n",
        "    def create_dataset_by_non_numeric_feature(self, feature_name, feature_values, df):\n",
        "        path = self.files_path + '/datasets/' + feature_name\n",
        "    \n",
        "        if not os.path.exists(path):\n",
        "            os.makedirs(path)\n",
        "        for value in feature_values:\n",
        "            df1 = df[df[feature_name]==value]\n",
        "            X = df1.drop(feature_name,1)\n",
        "            self.create_dataset_file(X, path, value)\n",
        "    \n",
        "    \n",
        "    def create_dataset_by_numeric_feature(self, feature_name, df):\n",
        "        path = self.files_path + '/datasets/' + feature_name\n",
        "        useRanking = False\n",
        "        if not os.path.exists(path):\n",
        "            os.makedirs(path)\n",
        "        \n",
        "        try:\n",
        "            df_num = pd.qcut(df[feature_name], 3, retbins = True)\n",
        "        except ValueError:\n",
        "            df['rank'] = df[feature_name].rank(method='first')\n",
        "            df_num = pd.qcut(df['rank'], 3, retbins = True)\n",
        "            cut_offs = df_num[1]\n",
        "            useRanking = True\n",
        "            \n",
        "        \n",
        "        cut_offs = df_num[1]\n",
        "        if useRanking:\n",
        "            df_low = df[(cut_offs[0] < df['rank']) & (df['rank'] <= cut_offs[1])]\n",
        "            df_low.pop('rank')\n",
        "            df_medium = df[(cut_offs[1] < df['rank']) & (df['rank'] <= cut_offs[2])] \n",
        "            df_medium.pop('rank')\n",
        "            df_high = df[(cut_offs[2] < df['rank']) & (df['rank'] <= cut_offs[3])] \n",
        "            df_high.pop('rank')\n",
        "            df_num = pd.cut(df[feature_name], 3, retbins = True)\n",
        "            cut_offs = df_num[1]\n",
        "            df.pop('rank')\n",
        "        else:\n",
        "            df_low = df[(cut_offs[0] < df[feature_name]) & (df[feature_name] <= cut_offs[1])]\n",
        "            df_medium = df[(cut_offs[1] < df[feature_name]) & (df[feature_name] <= cut_offs[2])] \n",
        "            df_high = df[(cut_offs[2] < df[feature_name]) & (df[feature_name] <= cut_offs[3])] \n",
        "        self.create_dataset_file(df_low, path, 'low')\n",
        "        self.create_dataset_file(df_medium, path, 'medium')\n",
        "        self.create_dataset_file(df_high, path, 'high')\n",
        "        self.intervals[feature_name] =  list(cut_offs)\n",
        "    \n",
        "    \n",
        "    def create_dataset_file(self, df, new_dataset_path, file_name=''):\n",
        "        if not os.path.isfile('%s/%s.csv'% (new_dataset_path,file_name)):\n",
        "            df.to_csv(r'%s/%s.csv'% (new_dataset_path,file_name), index=None)\n",
        "        else: # else it exists so append without writing the header\n",
        "            df.to_csv(r'%s/%s.csv'% (new_dataset_path,file_name), mode='a', header=False, index=None)\n",
        "            \n",
        "    def split_dataset(self, df, manually = False, contexts = [], datasets_path='', number_of_intervals = 3, intervals = {}):\n",
        "        X = df.copy()\n",
        "        if manually:\n",
        "            return\n",
        "        else:\n",
        "            for attribute in self.non_numeric_contexts:\n",
        "                self.encoders[attribute] = LabelEncoder()\n",
        "                enc = df[attribute].str.decode('utf-8')\n",
        "                X[attribute + '_encoded'] = self.encoders[attribute].fit_transform(df[attribute])\n",
        "                X.pop(attribute)\n",
        "\n",
        "            for attribute in self.contexts:\n",
        "                if np.issubdtype(df[attribute].dtype, np.number):\n",
        "                    self.create_dataset_by_numeric_feature(attribute, X)\n",
        "                else:\n",
        "                    values = X[attribute + '_encoded'].unique()\n",
        "                    self.create_dataset_by_non_numeric_feature(attribute + '_encoded', values, X)\n",
        "\n",
        "    def separate_encoded_columns(self, df):\n",
        "        temp = pd.DataFrame()\n",
        "        for column in self.non_numeric_contexts:\n",
        "            temp[column] = df.pop(column + '_encoded')\n",
        "        return temp, df\n",
        "    \n",
        "    def separate_non_numeric_columns(self, df):\n",
        "        temp = pd.DataFrame()\n",
        "        for column in self.non_numeric_contexts:\n",
        "            temp[column] = df.pop(column)\n",
        "        return temp, df\n",
        "    #training the contexts\n",
        "    def create_context_models(self, df):   \n",
        "    #create models for each of the contexts      \n",
        "        _path = self.files_path + '/models'\n",
        "        for attribute in self.contexts:\n",
        "\n",
        "        #if the feature is numerical\n",
        "            if np.issubdtype(df[attribute].dtype, np.number):\n",
        "                \n",
        "                scaler_high = preprocessing.StandardScaler()\n",
        "                scaler_low = preprocessing.StandardScaler()\n",
        "                scaler_medium = preprocessing.StandardScaler()\n",
        "                \n",
        "                dataset_path = self.files_path + '/datasets/'+ attribute\n",
        "                model_path = _path + '/' + attribute\n",
        "                if not os.path.exists(model_path):\n",
        "                    os.makedirs(model_path)\n",
        "                    \n",
        "        #we read in the datasets for each feature\n",
        "                data_high = pd.read_csv('%s/%s.csv'% (dataset_path,\"high\"))\n",
        "                data_medium = pd.read_csv('%s/%s.csv'% (dataset_path,\"medium\"))\n",
        "                data_low = pd.read_csv('%s/%s.csv'% (dataset_path,\"low\"))\n",
        "                \n",
        "        #make dataframes\n",
        "                df_high = pd.DataFrame(data_high)\n",
        "                df_low = pd.DataFrame(data_low)\n",
        "                df_medium = pd.DataFrame(data_medium)\n",
        "                \n",
        "        #pop the nominal encoded column and the target\n",
        "                y_high = df_high.pop(self.target_feature)\n",
        "                y_low = df_low.pop(self.target_feature)\n",
        "                y_medium = df_medium.pop(self.target_feature)\n",
        "                dropped_columns_high, reduced_df_high  = self.separate_encoded_columns(df_high)\n",
        "                dropped_columns_low, reduced_df_low = self.separate_encoded_columns(df_low)\n",
        "                dropped_columns_medium, reduced_df_medium = self.separate_encoded_columns(df_medium)\n",
        "                \n",
        "                names = reduced_df_high.columns\n",
        "                \n",
        "        #scale the values\n",
        "                scaled_df_high = scaler_high.fit_transform(reduced_df_high)\n",
        "                scaled_df_low = scaler_low.fit_transform(reduced_df_low)\n",
        "                scaled_df_medium = scaler_medium.fit_transform(reduced_df_medium)\n",
        "                \n",
        "                df_high = pd.DataFrame(scaled_df_high, columns=names)\n",
        "                df_low = pd.DataFrame(scaled_df_low, columns=names)\n",
        "                df_medium = pd.DataFrame(scaled_df_medium, columns=names)\n",
        "                \n",
        "                df_high = pd.concat([df_high,dropped_columns_high], axis=1).reindex(df_high.index)\n",
        "                df_low = pd.concat([df_low,dropped_columns_low], axis=1).reindex(df_low.index)\n",
        "                df_medium = pd.concat([df_medium,dropped_columns_medium], axis=1).reindex(df_medium.index)\n",
        "                \n",
        "            \n",
        "        #create models and train them\n",
        "                high_model = RandomForestClassifier(max_depth=2, random_state=0)\n",
        "                low_model = RandomForestClassifier(max_depth=2, random_state=0)\n",
        "                medium_model = RandomForestClassifier(max_depth=2, random_state=0)\n",
        "                \n",
        "                high_model.fit(df_high, y_high)\n",
        "                low_model.fit(df_low, y_low)\n",
        "                medium_model.fit(df_medium, y_medium)\n",
        "                \n",
        "                filename_high = model_path + '/high.sav'\n",
        "                filename_low = model_path + '/low.sav'\n",
        "                filename_medium = model_path + '/medium.sav'\n",
        "                #write the models down in the provided path\n",
        "                pickle.dump(high_model, open(filename_high, 'wb'))\n",
        "                pickle.dump(low_model, open(filename_low, 'wb'))\n",
        "                pickle.dump(medium_model, open(filename_medium, 'wb'))\n",
        "                \n",
        "            else:\n",
        "    #if the feature is nominal\n",
        "                dataset_path = self.files_path + '/datasets/' + attribute + '_encoded'\n",
        "                model_path = _path + '/' + attribute + '_encoded'\n",
        "                values = df[attribute].unique()\n",
        "                if not os.path.exists(model_path):\n",
        "                    os.makedirs(model_path)\n",
        "                    \n",
        "        #foreach class in the nominal feature\n",
        "                for val in values:\n",
        "                    \n",
        "                    if isinstance(val, str):\n",
        "                        val_enc = str(self.encoders[attribute].transform([val])).strip(\"[]\")\n",
        "                    else:\n",
        "                        val_enc = str(self.encoders[attribute].transform([val.decode('utf-8')])).strip(\"[]\")\n",
        "                    \n",
        "                    #read in dataset\n",
        "                    data_val = pd.read_csv('%s/%s.csv'% (dataset_path,val_enc))\n",
        "                    df_val = pd.DataFrame(data_val)\n",
        "                    y_train = df_val.pop(self.target_feature)\n",
        "                    \n",
        "                    names = df_val.columns\n",
        "                    \n",
        "                    #scale the values\n",
        "                    scaler = preprocessing.StandardScaler()\n",
        "                    df_sc_val = scaler.fit_transform(df_val)\n",
        "                    \n",
        "                    df_val = pd.DataFrame(df_sc_val, columns=names)\n",
        "                    if not isinstance(val, str):\n",
        "                        val_name = val.decode(\"utf-8\")\n",
        "                    else:\n",
        "                        val_name = val\n",
        "                \n",
        "            #create and train the model\n",
        "                    model = RandomForestClassifier(max_depth=2, random_state=0)\n",
        "                    model.fit(df_val, y_train)\n",
        "                    filename = model_path + '/' + val_name +'.sav'\n",
        "                \n",
        "            #write the model down in a file\n",
        "                \n",
        "                    pickle.dump(model, open(filename, 'wb')) \n",
        "                    \n",
        "    def create_test_set(self, df):\n",
        "        \n",
        "        for non_num in self.non_numeric_contexts:          \n",
        "            s = df[non_num].loc[4]\n",
        "            if not isinstance(s, str):\n",
        "                df[non_num] = df[non_num].str.decode('utf-8')\n",
        "   \n",
        "            df[non_num + '_encoded'] = self.encoders[non_num].fit_transform(df[non_num])\n",
        "        y_test = df.pop(self.target_feature)\n",
        "        return (df, y_test)\n",
        "                \n",
        "    def make_predictions(self, X_test):\n",
        "        _path = self.files_path + '/models'\n",
        "#path to models\n",
        "        predictions = pd.DataFrame()\n",
        "        X_test.index = list(range(len(X_test)))\n",
        "        test_copy = X_test.copy()\n",
        "        #get un-encoded column to use it to get the right model for the nominal class\n",
        "        non_numeric_columns, test_copy = self.separate_non_numeric_columns(test_copy)\n",
        "        #scle the test dataset \n",
        "        encoded, non_encoded = self.separate_encoded_columns(test_copy)\n",
        "        scaler = preprocessing.StandardScaler()\n",
        "        scaled_df = scaler.fit_transform(non_encoded)\n",
        "        names = non_encoded.columns  \n",
        "        scaled_df = pd.DataFrame(scaled_df, columns = names)\n",
        "        scaled_df = pd.concat([scaled_df,encoded], axis=1).reindex(scaled_df.index)\n",
        "        #foeach context\n",
        "        for attribute in self.contexts:\n",
        "            print(attribute)\n",
        "            array = np.array([])  \n",
        "        #if its a numerical context\n",
        "            if np.issubdtype(df[attribute].dtype, np.number):                   \n",
        "            #foreach row of the test set\n",
        "                for row_index in range(len(X_test)):\n",
        "                    model_path = _path + '/' + attribute\n",
        "                #get value of the row for the current attribute\n",
        "                    row_val = X_test.loc[row_index, attribute]\n",
        "                #get row for which we are predicting, but from the scaled test set\n",
        "                    row = scaled_df.iloc[row_index].to_numpy()\n",
        "                    \n",
        "                    filename_high = model_path + '/high.sav'\n",
        "                    filename_low = model_path + '/low.sav'\n",
        "                    filename_medium = model_path + '/medium.sav'\n",
        "                    intervals = self.intervals[attribute]\n",
        "                    #check within which interval the attribute value belongs so we know which model to load \n",
        "                    if ((intervals[0] < row_val) & (row_val <= intervals[1])):\n",
        "                        loaded_model = pickle.load(open(filename_low, 'rb'))\n",
        "                    if ((intervals[1] < row_val) & (row_val <= intervals[2])):\n",
        "                        loaded_model = pickle.load(open(filename_medium, 'rb'))\n",
        "                    if ((intervals[2] < row_val) & (row_val <= intervals[3])):\n",
        "                        loaded_model = pickle.load(open(filename_high, 'rb'))\n",
        "                    #predict value for the current row\n",
        "                    prediction = loaded_model.predict(pd.DataFrame(row).T)\n",
        "                    array = np.append(array, prediction)\n",
        "                    #save value to predictions dictionary\n",
        "                predictions[attribute] = array\n",
        "            else: \n",
        "                #Foreach row of X_test_non_num dataset\n",
        "                X_test_non_num = X_test.drop(attribute, 1)\n",
        "                scaled_df_non_num = scaled_df.drop(attribute,1)\n",
        "                for row_index in range(len(X_test_non_num)):\n",
        "                #get name from the non encoded version of the column, from the test set, to know which model we need to load\n",
        "                #this is from the non scaled version of the test set\n",
        "                    s = non_numeric_columns[attribute].loc[row_index]\n",
        "                    \n",
        "                    if isinstance(s, str):\n",
        "                        model_name = non_numeric_columns[attribute].loc[row_index]\n",
        "                    else:\n",
        "                        model_name = str(non_numeric_columns[attribute].loc[row_index].decode('utf-8'))\n",
        "                        \n",
        "                #get row from the scaled version of the test set\n",
        "                    row = scaled_df_non_num.iloc[row_index].to_numpy()\n",
        "                    filename = _path + '/' + attribute + '_encoded' + '/' + model_name +'.sav'\n",
        "                #load model\n",
        "                    loaded_model = pickle.load(open(filename, 'rb'))\n",
        "                #make prediction\n",
        "                    prediction = loaded_model.predict(pd.DataFrame(row).T)\n",
        "                    array = np.append(array, prediction)\n",
        "                predictions[attribute] = array\n",
        "        return predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "853hGrF5tQh_",
        "outputId": "ef45243d-fbed-465a-da9d-224289cf75f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        }
      },
      "source": [
        "data = pd.read_csv(os.getcwd() +'/bank-additional-full.csv', sep=';')\n",
        "df = pd.DataFrame(data)\n",
        "y = df.pop('y')\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.33, random_state=42)\n",
        "train = pd.concat([X_train,y_train], axis=1).reindex(X_train.index)\n",
        "test = pd.concat([X_test,y_test], axis=1).reindex(X_test.index)\n",
        "context_based_rand_forest_ensemble = ContextBasedRandomForestClassifierEnsemble('y', 'bank-additional-full')\n",
        "\n",
        "context_based_rand_forest_ensemble.set_contexts(train)\n",
        "            \n",
        "context_based_rand_forest_ensemble.split_dataset(train)\n",
        "\n",
        "context_based_rand_forest_ensemble.create_context_models(train)\n",
        "\n",
        "X_test_proc, y_test_proc = context_based_rand_forest_ensemble.create_test_set(test)\n",
        "\n",
        "predictions = context_based_rand_forest_ensemble.make_predictions(X_test_proc)       \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "age\n",
            "job\n",
            "marital\n",
            "education\n",
            "default\n",
            "housing\n",
            "loan\n",
            "contact\n",
            "month\n",
            "day_of_week\n",
            "duration\n",
            "campaign\n",
            "pdays\n",
            "previous\n",
            "poutcome\n",
            "emp.var.rate\n",
            "cons.price.idx\n",
            "cons.conf.idx\n",
            "euribor3m\n",
            "nr.employed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSiqrhO4brz3",
        "outputId": "3a58e5f1-db12-4fee-e46b-e1a9d5a76ffe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        }
      },
      "source": [
        "predictions"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>job</th>\n",
              "      <th>marital</th>\n",
              "      <th>education</th>\n",
              "      <th>default</th>\n",
              "      <th>housing</th>\n",
              "      <th>loan</th>\n",
              "      <th>contact</th>\n",
              "      <th>month</th>\n",
              "      <th>day_of_week</th>\n",
              "      <th>duration</th>\n",
              "      <th>campaign</th>\n",
              "      <th>pdays</th>\n",
              "      <th>previous</th>\n",
              "      <th>poutcome</th>\n",
              "      <th>emp.var.rate</th>\n",
              "      <th>cons.price.idx</th>\n",
              "      <th>cons.conf.idx</th>\n",
              "      <th>euribor3m</th>\n",
              "      <th>nr.employed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13588</th>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13589</th>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13590</th>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13591</th>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13592</th>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>13593 rows × 20 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      age job marital  ... cons.conf.idx euribor3m nr.employed\n",
              "0      no  no      no  ...            no        no          no\n",
              "1      no  no      no  ...            no        no          no\n",
              "2      no  no      no  ...            no        no          no\n",
              "3      no  no      no  ...            no        no          no\n",
              "4      no  no      no  ...            no        no          no\n",
              "...    ..  ..     ...  ...           ...       ...         ...\n",
              "13588  no  no      no  ...            no        no          no\n",
              "13589  no  no      no  ...            no        no          no\n",
              "13590  no  no      no  ...            no        no          no\n",
              "13591  no  no      no  ...            no        no          no\n",
              "13592  no  no      no  ...            no        no          no\n",
              "\n",
              "[13593 rows x 20 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07ZSAfXj2uNz",
        "outputId": "e71431ac-1bcd-4f4d-832f-7411e7a88630",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y = []\n",
        "for col in predictions.columns:\n",
        "  calculated_y = predictions[col]\n",
        "  y.append(round(accuracy_score(y_test_proc, calculated_y),2)*100)\n",
        "yy = np.mean(y)\n",
        "print('CBM predicts with ' + str(yy) + '% accuracy')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CBM predicts with 89.8% accuracy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLXEcSljVVOp"
      },
      "source": [
        "#Single model\n",
        "Dolu na istiot dataset, obraboten na ist nacin se trenira single RandomForestClassifier model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pn6Z-Ozwb8Md",
        "outputId": "c540988e-99ef-4380-b968-42d400faf112",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "data = pd.read_csv(os.getcwd() +'/bank-additional-full.csv', sep=';')\n",
        "bank_df = pd.DataFrame(data)\n",
        "y = bank_df.pop('y')\n",
        "\n",
        "labelencoder_X = LabelEncoder()\n",
        "bank_df['contact']     = labelencoder_X.fit_transform(bank_df['contact']) \n",
        "bank_df['month']       = labelencoder_X.fit_transform(bank_df['month']) \n",
        "bank_df['day_of_week'] = labelencoder_X.fit_transform(bank_df['day_of_week']) \n",
        "bank_df['job']      = labelencoder_X.fit_transform(bank_df['job']) \n",
        "bank_df['marital']  = labelencoder_X.fit_transform(bank_df['marital']) \n",
        "bank_df['education']= labelencoder_X.fit_transform(bank_df['education']) \n",
        "bank_df['default']  = labelencoder_X.fit_transform(bank_df['default']) \n",
        "bank_df['housing']  = labelencoder_X.fit_transform(bank_df['housing']) \n",
        "bank_df['loan']     = labelencoder_X.fit_transform(bank_df['loan']) \n",
        "bank_df['poutcome']  = labelencoder_X.fit_transform(bank_df['poutcome']) \n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(bank_df, y, test_size=0.33, random_state=42)\n",
        "\n",
        "sc_X = preprocessing.StandardScaler()\n",
        "X_train = sc_X.fit_transform(X_train)\n",
        "X_test = sc_X.fit_transform(X_test)\n",
        "\n",
        "rfc = RandomForestClassifier(max_depth=2, random_state=0)\n",
        "rfc.fit(X_train, y_train)\n",
        "rfcpred = rfc.predict(X_test)\n",
        "\n",
        "print(confusion_matrix(y_test, rfcpred ))\n",
        "print(round(accuracy_score(y_test, rfcpred),2)*100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[12048     8]\n",
            " [ 1476    61]]\n",
            "89.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lg9ku1lWx1EN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}